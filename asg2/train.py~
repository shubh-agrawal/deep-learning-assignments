'''
Deep Learning Programming Assignment 1
--------------------------------------
Name: Agrawal Shubh Mohan
Roll No.: 14ME30003

======================================
Complete the functions in this file.
Note: Do not change the function signatures of the train
and test functions
'''
import numpy as np
import random
import mnist_loader

def sigmoid(z):
    "sigmoid function"
    return 1.0/(1.0+np.exp(-z))


def sigmoid_prime(z):
    "Derivative of sigmoid"
    return sigmoid(z)*(1-sigmoid(z))



class neuralNetwork(object):


    def __init__(self, layers, test_on = 0):
         self.num_layers = len(layers)
         self.layers = layers
         if test_on == 0:
             self.weights = [np.random.randn(y, x) for x,y in zip(layers[:-1], layers[1:])]
             self.biases = [np.random.randn(y, 1) for y in layers[1:]]
         else:
             self.weights = np.load("weights/weights.npy")
             self.biases = np.load("weights/biases.npy")


    def feedforward(self, outputAct):        
        for b, w in zip(self.biases, self.weights):
            #print w.shape, b.shape
            outputAct = sigmoid(np.dot(w, outputAct)+b)
        return outputAct



    def gradientDescent(self, all_data, epochs, mini_batch_size, lr, n_test=0):
        
        if n_test != 0 :
            test_data = all_data[:n_test]
        
        training_data = all_data[n_test:]    
        n_train = len(training_data)

        for j in xrange(epochs):
            random.shuffle(training_data)
            mini_batches = [ training_data[k:k+mini_batch_size] for k in xrange(0, n_train, mini_batch_size)]
            for mini_batch in mini_batches:
                self.update_mini_batch(mini_batch, lr)
            if n_test !=0:
                print "Epoch {0}: {1} / {2}".format(j, self.evaluate(test_data), n_test)
            else:
                print "Epoch {0} complete".format(j)



    def update_mini_batch(self, mini_batch, lr):
        nabla_b = [np.zeros(b.shape) for b in self.biases]
        nabla_w = [np.zeros(w.shape) for w in self.weights]
        for x, y in mini_batch:
            delta_nabla_b, delta_nabla_w = self.backprop(x, y)
            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]
            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]
        self.weights = [w-(lr/len(mini_batch))*nw
                        for w, nw in zip(self.weights, nabla_w)]
        self.biases = [b-(lr/len(mini_batch))*nb
                       for b, nb in zip(self.biases, nabla_b)]



    def backprop(self, x, y):
        nabla_b = [np.zeros(b.shape) for b in self.biases]
        nabla_w = [np.zeros(w.shape) for w in self.weights]
        
        # feed it forward
        activation = x
        activations = [x] # list to store all the activations, layer by layer
        zs = [] # list to store all the z vectors, layer by layer
        
        for b, w in zip(self.biases, self.weights):
            z = np.dot(w, activation)+b
            zs.append(z)
            activation = sigmoid(z)
            activations.append(activation)
        
        # backward go
        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])
        nabla_b[-1] = delta
        nabla_w[-1] = np.dot(delta, activations[-2].transpose())
        for l in xrange(2, self.num_layers):
            z = zs[-l]
            sp = sigmoid_prime(z)
            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp
            nabla_b[-l] = delta
            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())
        return (nabla_b, nabla_w)

    def evaluate(self, test_data):        
        test_results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in test_data]   
        return sum(int(x == y) for (x, y) in test_results)


    def cost_derivative(self, output_activations, y):
        #print output_activations
        return (output_activations-y)


def train(trainX, trainY):

    n_validation = 10000
    # Convert to one hot vector
    labels = np.zeros((len(trainY), 10, 1))
    labels[np.arange(len(trainY)), trainY] = 1.0
     
    trainX = np.array(trainX).reshape(len(trainY), 784, 1).astype(np.float)
    trainX = trainX/255.0
    training_data = zip(trainX, labels)
    
    nNet = neuralNetwork([784, 50, 10])
    nNet.gradientDescent(training_data, 2, 10, 3.0, n_validation)
    
    np.save("weights/weights.npy", nNet.weights)
    np.save("weights/biases.npy", nNet.biases)            

def test(testX):
    testX = np.array(testX).reshape(len(testX), 784, 1).astype(np.float)
    testX = testX/255.0
    test_net = neuralNetwork([784, 50, 10], 1)   
    test_result = [np.argmax(test_net.feedforward(x)) for x in testX] 

    return test_result
