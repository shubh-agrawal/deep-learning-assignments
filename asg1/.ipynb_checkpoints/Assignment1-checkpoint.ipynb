{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Assignment 1\n",
    "\n",
    "- Do not submit your answers as images where text or an equation is expected. You might be evaluated with a zero.\n",
    "- Use mathlatex (latex notations) to type math equations\n",
    "- If at all you feel you need to add some diagram or illustration, use relative path to add them as image and make sure you include them in the zipped archive that you will be submitting in the moodle\n",
    "- Name your notebook and the zip as < rollno >_A1.zip. For example if you are roll number 13CS60R12 then submit the zip as 13CS60R12_A1.zip\n",
    "\n",
    "- The marks for the individual questions will be decided later\n",
    "- Double click on the cells where it is written \"Ans. Write your answer here.\". Markdown syntax needs to be followed while writing the answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. \n",
    "We are given a dataset $(X,Y)$.  Which among the following classifiers will contain sufficient information that allows the calculation of joint probability of the features and the label in the dataset? Justify your answer for each of the classifer.\n",
    "If $X = (X_1,X_2,X_3,X_4)$ then you need to calculate $P(X_1,X_2,X_3,X_4,Y)$.\n",
    "\n",
    " - Linear Regression\n",
    " - Logisitc Regression\n",
    " - Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "\n",
    "* For linear regression,\n",
    "\n",
    "We can't find any $P(X)$ as the method simply consists of matrix algebra.\n",
    "\n",
    "* For logisitic regression,\n",
    "\n",
    "Similarly, we cannot fint a probablity $P(X)$. Hence, computation of joint probability is not possible.\n",
    "\n",
    "* For Gaussian Naive Bayes,\n",
    "\n",
    "Using the chain rule, we can estimate $P(X_1, X_2, X_3, Y) = P(Y)P(X_1|Y)P(X_2|Y)P(X_3|Y)P(X_4|Y) $\n",
    "The probabilities on right hand side are also available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2a. \n",
    "For two discrete-valued distributions $P(X), Q(X)$, K-L Divergence is defined as\n",
    " \n",
    "$$ KL(P||Q) = \\sum_x P(x)log\\frac{P(x)}{Q(x)}$$\n",
    "\n",
    "where P(x) > 0. \n",
    "\n",
    "Prove the following $$ \\forall P,Q~~ KL(P||Q) \\geq 0 $$ and\n",
    "$$ KL(P||Q) ~ = ~ 0 ~ iff ~ P ~ = ~ Q$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans.\n",
    "- 1)\n",
    "$$ KL(P||Q) = \\sum_x P(x)log\\frac{P(x)}{Q(x)} $$\n",
    "\n",
    "$$ KL(P||Q) = - \\sum_x P(x)\\bigg(log\\frac{Q(x)}{P(x)} \\bigg) $$\n",
    "\n",
    "$$ KL(P||Q) \\geq - log\\bigg(\\sum_x P(x)\\frac{Q(x)}{P(x)} \\bigg) $$\n",
    "\n",
    "$$ KL(P||Q) \\geq - log(1) $$\n",
    "\n",
    "$$ KL(P||Q) \\geq 0 $$\n",
    "\n",
    "- 2) if P = Q then \n",
    "$$ KL(P||Q) = \\sum_x P(x)log\\frac{P(x)}{Q(x)} $$\n",
    "\n",
    "$$ KL(P||Q) = \\sum_x P(x)log(1) $$\n",
    "\n",
    "$$ KL(P||Q) = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "## Q2b.\n",
    "The KL-Divergence between two conditional distributions $P(X|Y),Q(X|Y)$ is\n",
    "$$ KL(P(X|Y)||Q(X|Y)) ~ = ~ \\sum_y P(y) \\bigg( \\sum_x P(x|y)log\\frac{P(x|y)}{Q(x|y)} \\bigg)$$\n",
    "\n",
    "\n",
    "Prove the following chain rule for KL Divergence:\n",
    "$$ KL(P(X|Y)||Q(X|Y) = KL(P(X)||Q(X)) + KL(P(Y|X)||Q(Y|X)) $$\n",
    "\n",
    "Ans.\n",
    "\n",
    "$$ KL(P(X|Y)||Q(X|Y)) ~ = ~ \\sum_y P(y) \\bigg( \\sum_x P(x|y)log\\frac{P(x|y)}{Q(x|y)} \\bigg)$$\n",
    "\n",
    "$$ KL(P(X|Y)||Q(X|Y)) ~ = ~ \\sum_y P(y) \\bigg( \\sum_x P(x|y)log\\frac{P(y|x)P(x)Q(y)}{Q(y|x)Q(x)P(y)} \\bigg)$$\n",
    "\n",
    "$$ KL(P(X|Y)||Q(X|Y)) ~ = ~ \\sum_y P(y) \\sum_x \\frac{P(y|x)P(x)}{P(y)}log\\frac{P(y|x)}{Q(y|x)} + \\sum_y P(y) \\sum_x P(x|y)log\\frac{P(x)Q(y)}{Q(x)P(y)} $$\n",
    "\n",
    "$$ KL(P(X|Y)||Q(X|Y)) ~ = ~ \\sum_x \\sum_y P(x, y)log\\frac{P(y|x)}{Q(y|x)} + \\sum_x P(x)log\\frac{P(x)}{P(y)} $$\n",
    "\n",
    "$$ KL(P(X|Y)||Q(X|Y)) ~ = ~ \\sum_x P(x)\\sum_y P(y|x)log\\frac{P(y|x)}{Q(y|x)} + \\sum_x P(x)log\\frac{P(x)}{P(y)} $$\n",
    "\n",
    "$$ KL(P(X|Y)||Q(X|Y) = KL(P(Y|X)||Q(Y|X)) + KL(P(X)||Q(X)) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. \n",
    "What is the role of the activation function in a neural network? What would happen if you just used the identify function as an activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans.\n",
    "\n",
    "It is important that the nerual network is able to learn complex functions that accounts for most of the machine learning problems. A complex function is possible only with integration of variety of activation functions that must also be complex in nature. With this the nerual network will be able fit the dataset and would be able to better generalize. Contrary, if the activation function is a identity function then the overall neural network will be able to learn only a simple function like linear ones and hence won't be able to fit to the given dataset (also called as underfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. \n",
    "Assume a friend of yours recently got diogonised for a rare disease and it is given that the testing methods for this disease are correct 99 percent of the time. You did some googling and found that the chances of the disease to occur randomly in the general population is only one of every 10,000 people. What are the chances that your friend actually have the disease?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "\n",
    "$$ P(disease) = 0.0001 $$\n",
    "$$ P(test Positive | disease) = 0.99 $$\n",
    "$$ P(test Postive | no Disease) = 0.01 $$\n",
    "$$ P(test Negative | no Disease) = 0.99 $$\n",
    "\n",
    "$$ P(disease | test Positive) = ?? $$\n",
    "\n",
    "\n",
    "Using bayes theorem, \n",
    "\n",
    "$$ P(disease | test Positive) = \\frac{P(test Positive | disease) P(disease)}{P(test Positive | disease)P(disease) + P(test Positive | no Disease)P(no Disease)} $$\n",
    "\n",
    "$$ P(disease | test Positive) = \\frac{0.99x0.0001}{(0.99x0.0001 + 0.01x0.9999)} $$\n",
    "\n",
    "$$ P(disease | test Positive) = 0.00980 $$  \n",
    "\n",
    "Probability that he actually has disease is 0.00980"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "## Q5. \n",
    "How exactly is the training of structured perceptron different from that of a perceptron? Explain how we can solve argmax problem for sequences in the context of a structured perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "\n",
    "The Output of a structered perceptron is a label sequence, which can mismatch with ground truth for a large combination. Unlike regular perceptron which can be trained for a set of label options, this type of perceptron can have a large pool of label sequences with which loss has to be calculated.\n",
    "\n",
    "There can be two views:\n",
    "1. Classification: find the correl label sequence\n",
    "2. Ranking: find the most compatible label sequence\n",
    "\n",
    "In structered perceptron, more importance is given to ranking as against to classical approach of classification\n",
    "\n",
    "The only difficulty in training is prediciting, (the argmax problem)\n",
    "\n",
    "$$ \\hat{y} = \\underset{y\\in y^N}{\\arg \\max} \\mathbf{w}.\\phi(x, y) $$ \n",
    "\n",
    "This problem will not be tractable in the general case.\n",
    "However, for very specific $y$ and very specific $\\phi$, one can employ dynamic programming algorithms or integer programming algorithms to find efficient solutions. In particular, if $\\phi$ decomposes over the vector representation of $y$ such that no feature depends on elements of $y$ that are more than k positions away, then the Viterbi algorithm can be used to solve the argmax problem in time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------\n",
    "\n",
    "## Q6. \n",
    "We already know differentiation in the context of univariate real valued functions (input $\\in \\mathbb{R}$, output $\\in \\mathbb{R}$). For e.g $\\frac{d}{dx}(x^2 +x) = 2x + 1$.\n",
    "\n",
    "Now we define differentiation in the context of matrices and vectors. Consider a function $f(\\mathbf{x}) = \\mathbf{y}$, where $\\mathbf{x} = (x_1, x_2, \\dots, x_n)^T \\in \\mathbb{R}^n$ and $\\mathbf{y} = (y_1, y_2, \\dots, y_m)^T \\in \\mathbb{R}^m$ are vectors. We define the derivative of $f(\\mathbf{x})$ wrt $\\mathbf{x}$ as\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\mathbf{x}} f(\\mathbf{x}) = \n",
    "    \\begin{bmatrix}\n",
    "    \\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_1}{\\partial x_2} & \\cdots & \\frac{\\partial y_1}{\\partial x_n}\\\\[2mm]\n",
    "\t\\frac{\\partial y_2}{\\partial x_1} & \\frac{\\partial y_2}{\\partial x_2} & \\cdots & \\frac{\\partial y_2}{\\partial x_n}\\\\\n",
    "\t\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\t\\frac{\\partial y_m}{\\partial x_1} & \\frac{\\partial y_m}{\\partial x_2} & \\cdots & \\frac{\\partial y_m}{\\partial x_n}\\\\\n",
    "\t\\end{bmatrix}\n",
    "\\text{ or } \\left[\\frac{\\partial}{\\partial \\mathbf{x}} f(\\mathbf{x})\\right]_{ij} = \\frac{\\partial y_i}{\\partial x_j}\n",
    "$$\n",
    "\n",
    "If $\\mathbf{x}$ is a scalar (denote by $x$) then we define the derivative as a vector of elementwise derivatives given by\n",
    "$$\n",
    "    \\left[\\frac{\\partial}{\\partial x} f(x)\\right]_i = \\frac{\\partial y_i}{\\partial x}\n",
    "$$\n",
    "Similarly, if $\\mathbf{y}$ is a scalar (denote by $y$) then we define the derivative as,\n",
    "$$\n",
    "    \\left[\\frac{\\partial}{\\partial \\mathbf{x}} f(\\mathbf{x})\\right]_i = \\frac{\\partial y}{\\partial x_i}\n",
    "$$\n",
    "\n",
    "If $\\mathbf{y}$ is a scalar (denote by $y$) and $\\mathbf{x}$ is a matrix (denote by $X$), then we define the derivative as a matrix given by\n",
    "$$\n",
    "    \\left[\\frac{\\partial}{\\partial X} f(X)\\right]_{ij} = \\frac{\\partial y}{\\partial X_{ij}}\n",
    "$$\n",
    "\n",
    "Given that $A, X\\in \\mathbb{R}^{a\\times b}$ and $v, x\\in \\mathbb{R}^{b}$, show the following:\n",
    " - $\\frac{\\partial}{\\partial x} v^T x = \\frac{\\partial}{\\partial x} x^T v = v$\n",
    " - $\\frac{\\partial}{\\partial x} Ax = A$\n",
    " - If a=b $\\frac{\\partial}{\\partial x} x^TAx = Ax + A^Tx$\n",
    "\n",
    "Using the above results, show the following result (which is actually the solution to least squares linear regression)\n",
    "$$\n",
    "    \\underset{w}{\\arg \\min} \\| Xw-Y\\|_2^2 = (X^TX)^{-1}X^TY\n",
    "$$\n",
    "(Hint: $\\|v\\|_2^2 = v^Tv$. Write the above norm in this form, differentiate and equate to zero.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ans\n",
    "\n",
    "1) $\\frac{\\partial}{\\partial x} v^T x = \\frac{\\partial}{\\partial x} x^T v = v$\n",
    "\n",
    "#### Solution.\n",
    "\n",
    "Lets say $ w = v^T x $\n",
    "\n",
    "Here, $w$ is a scalar as $ w = v_1 x_1 + v_2 x_2 + \\dots + v_b x_b = \\sum_b x_i v_i $\n",
    "\n",
    "Similarly, $ x^T v = x_1 v_1 + x_2 v_2 + \\dots + x_b v_b = \\sum_b x_i v_i $\n",
    "\n",
    "Using a rule mentioned in quetion,\n",
    "\n",
    "$ \\frac{\\partial w}{\\partial x} = \\frac{\\partial w}{\\partial x_i} $\n",
    "\n",
    "$ \\frac{\\partial w}{\\partial x} = \\bigg( \\frac{\\partial w}{\\partial x_1} , \\frac{\\partial w}{\\partial x_2} , \\dots, \\frac{\\partial w}{\\partial x_b} \\bigg) $\n",
    "\n",
    "For any $ i\\in (1, b)$\n",
    "\n",
    "$ \\frac{\\partial \\sum_b x_i v_i}{\\partial x_i} = v_i $\n",
    "\n",
    "$ \\frac{\\partial w}{\\partial x} = ( v_1, v_2, \\dots, v_b) $\n",
    "\n",
    "Hence, $\\frac{\\partial}{\\partial x} v^T x = \\frac{\\partial}{\\partial x} x^T v = v $\n",
    "\n",
    "2) $\\frac{\\partial}{\\partial x} Ax = A$\n",
    "\n",
    "#### Solution.\n",
    "\n",
    "Lets say, $ \\mathbf{w} = Ax  \\text{ that is a } \\mathbb{R}^{a} \\text{ sized vector } $\n",
    "\n",
    "$$ \\mathbf{w} = Ax =\n",
    "    \\begin{bmatrix}\n",
    "    \\sum_b A_{1j} x_j \\\\[2mm]\n",
    "\t\\sum_b A_{2j} x_j \\\\\n",
    "\t\\vdots \\\\\n",
    "\t\\sum_b A_{aj} x_j \\\\\n",
    "\t\\end{bmatrix}\n",
    "    =\n",
    "    \\begin{bmatrix}\n",
    "    w_1 \\\\[2mm]\n",
    "\tw_2 \\\\\n",
    "\t\\vdots \\\\\n",
    "\tw_a \\\\\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now,\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\mathbf{x}} \\mathbf{w} = \n",
    "    \\begin{bmatrix}\n",
    "    \\frac{\\partial w_1}{\\partial x_1} & \\frac{\\partial w_1}{\\partial x_2} & \\cdots & \\frac{\\partial w_1}{\\partial x_b}\\\\[2mm]\n",
    "\t\\frac{\\partial w_2}{\\partial x_1} & \\frac{\\partial w_2}{\\partial x_2} & \\cdots & \\frac{\\partial w_2}{\\partial x_b}\\\\\n",
    "\t\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\t\\frac{\\partial w_a}{\\partial x_1} & \\frac{\\partial w_a}{\\partial x_2} & \\cdots & \\frac{\\partial w_a}{\\partial x_b}\\\\\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\mathbf{x}} \\mathbf{w} = \n",
    "    \\begin{bmatrix}\n",
    "    \\frac{\\partial \\sum_b A_{1j} x_j}{\\partial x_1} & \\frac{\\partial \\sum_b A_{1j} x_j}{\\partial x_2} & \\cdots & \\frac{\\partial \\sum_b A_{1j} x_j}{\\partial x_b}\\\\[2mm]\n",
    "\t\\frac{\\partial \\sum_b A_{2j} x_j}{\\partial x_1} & \\frac{\\partial \\sum_b A_{2j} x_j}{\\partial x_2} & \\cdots & \\frac{\\partial \\sum_b A_{2j} x_j}{\\partial x_b}\\\\\n",
    "\t\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\t\\frac{\\partial \\sum_b A_{aj} x_j}{\\partial x_1} & \\frac{\\partial \\sum_b A_{aj} x_j}{\\partial x_2} & \\cdots & \\frac{\\partial \\sum_b A_{aj} x_j}{\\partial x_b}\\\\\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And, $\\frac{\\partial \\sum_b A_{ij} x_j}{\\partial x_j} = A_{ij} $\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\mathbf{x}} \\mathbf{w} = \n",
    "    \\begin{bmatrix}\n",
    "    A_{11} & A_{12} & \\cdots & A_{1b}\\\\[2mm]\n",
    "\tA_{21} & A_{22} & \\cdots & A_{2b}\\\\\n",
    "\t\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\tA_{a1} & A_{a2} & \\cdots & A_{ab}\\\\\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$\n",
    "\\frac{\\partial}{\\partial \\mathbf{x}} Ax = \\frac{\\partial}{\\partial \\mathbf{x}} \\mathbf{w} = A\n",
    "$\n",
    "\n",
    "\n",
    "3) $\\frac{\\partial}{\\partial x} x^TAx = Ax + A^Tx$\n",
    "\n",
    "#### Solution\n",
    "\n",
    "Let $w = \\mathbf{x^TAx} $\n",
    "and $ a=b=n $\n",
    "\n",
    "Hence, $w$ is a scalar\n",
    "\n",
    "$$\n",
    "w = [x_1, x_2, \\dots, x_n]   \n",
    "    \\begin{bmatrix}\n",
    "    A_{11} & A_{12} & \\cdots & A_{1n}\\\\[2mm]\n",
    "\tA_{21} & A_{22} & \\cdots & A_{2n}\\\\\n",
    "\t\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\tA_{n1} & A_{n2} & \\cdots & A_{nn}\\\\\n",
    "\t\\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "    x_1 \\\\[2mm]\n",
    "\tx_2 \\\\\n",
    "\t\\vdots \\\\\n",
    "\tx_n \\\\\n",
    "\t\\end{bmatrix}    \n",
    "$$\n",
    "\n",
    "$$\n",
    "w = [x_1, x_2, \\dots, x_n]\n",
    "    \\begin{bmatrix}\n",
    "    \\sum_n A_{1j} x_j \\\\[2mm]\n",
    "\t\\sum_n A_{2j} x_j \\\\\n",
    "\t\\vdots \\\\\n",
    "\t\\sum_n A_{nj} x_j \\\\\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "w = \\sum_n \\sum_n A_{ij} x_i x_j\n",
    "$$\n",
    "\n",
    "Now,\n",
    "\n",
    "$$ \\frac{\\partial w}{\\partial x_k} = \\sum_n A_{ik} x_i + \\sum_n A_{kj} x_j $$ \n",
    "\n",
    "$$ \\frac{\\partial w}{\\partial x_k} = A^T x + A x $$\n",
    "\n",
    "$$ \\frac{\\partial \\mathbf{x^T Ax}}{\\partial x} = A^T x + A x $$\n",
    "\n",
    "\n",
    "To Prove:\n",
    "\n",
    "- $ \\underset{w}{\\arg \\min} \\| Xw-Y\\|_2^2 = (X^TX)^{-1}X^TY $\n",
    "\n",
    "#### Solution\n",
    "\n",
    "$$ \\frac{\\partial \\| Xw-Y\\|_2^2}{\\partial w} = 0 $$\n",
    "\n",
    "$$ \\frac{\\partial (Xw-Y)^T (Xw-Y)}{\\partial w} = 0 $$\n",
    "\n",
    "$$ \\frac{\\partial (w^t X^T - Y^T)(Xw - Y)}{\\partial w} = 0 $$\n",
    "\n",
    "$$ \\frac{\\partial (w^t X^T X w - w^T X^T Y - Y^T X w + Y^T Y)}{\\partial w} = 0 $$\n",
    "\n",
    "$$ \\frac{\\partial w^t X^T X w}{\\partial w} - \\frac{\\partial w^T X^T Y}{\\partial w} - \\frac{\\partial Y^T X w}{\\partial w} + \\frac{\\partial Y^T Y}{\\partial w} = 0 $$\n",
    "\n",
    "From matrix calculus rules,\n",
    "\n",
    "$$ X^T X w + X^T X w - X^T Y - X^T Y = 0 $$\n",
    "\n",
    "$$ X^T X w = X^T Y $$\n",
    "\n",
    "Take inverse multipliers on both sides\n",
    "\n",
    "$$ w = (X^T X)^{-1} X^T Y $$\n",
    "\n",
    "Hence proved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------\n",
    "## Q7a \n",
    "You are given a Neural Network model which claims to detect between Huskies and Wolves. You are also shown the predictions of the model on 10 held-out images. \n",
    "\n",
    "The results show amongs 10 images (5 each from both the classes), it mis-predicts the 2 cases (1 each from each of the class - 6th and 9th images) of the 10 images.\n",
    "\n",
    "\n",
    "- 1) How much do you trust the model? Give a subject evlaution of the model\n",
    "- 2) What do you think is the system learning?\n",
    "\n",
    "\n",
    "This particular question does not look for the exact answer and rather this question wants to test your thinking and reasoning capacity. So try to come up with multiple possible explanations. <i>The subsequent question will show what exactly was the neural network learning, and hence it is implied that we expect different answers from what is given below. So attempt the next question only after finishing this question</i> \n",
    "\n",
    "![Wolf or Huskies](images/7a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans.\n",
    "\n",
    "- 1) As far the Generalization capability of the model is concerned, the model predicts 8 cases correct out of 10 test cases. This implies the accuracy of the model is 80% or simply 0.8. So for any future predicitions we can trust he model to be right for approximately 80%. Note that any term like \"atleast 80%\" or \"atmost 80%\" is not used as it will be still unprobable if the model can give 100% accuracy or 50% accuracy for future predicitions. However, the accuracy obtained on the test cases can represent a good measure for performance metric for the model.\n",
    "- 2) From the images and their corresponding predicition, it is clearly understood that the model is trying to give result by giving more importance to background scene. In the given images, the wolves are found to be in snow clad region which gives their background scene a bright essence. Contrary to that, the background of husky is found to be mostly dry barren land region. The model predicts an image with husky in 9th image as its background seems to be dry and barren. Similarly in 6th image, husky is predicted to be a wolf as the background for the images seems to be snow claded. Interestingly, whenever husky and wolf are switched in their respective environement, the system fails to give the correct result.\n",
    "- The model is trying to learn generalized feature which can exist simultaneosly for both wolf and husky, and are creating problem for diffrentiating. For this problem statement, the model can be trusted if it looks more into deep features rather than generic ones to compute the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "## 7b) \n",
    "Given below are the previously shown images along with its corresponding reconstructed portions obtained from the neural network.\n",
    "\n",
    "The non-gray parts on the reconstructed images are the parts of the image that the neural network thinks are the most important in making the predictions. With the new evidence (assuming you answered previous question without looking into this), please reanswer the above question\n",
    "- 1) How much do you trust the model? Give a subject evlaution of the model\n",
    "- 2) What do you think is the system learning?\n",
    "\n",
    "\n",
    "![Double click and remove the exclamation mark inside the parenthesis to see the image](images/7b.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans.\n",
    "\n",
    "- After looking at the features that the neural network thinks important, it is evident that they seem to be pretty generic in nature. The features include the background scene information (color, texture) and some body patches of the animal. It is possible that a wolf exists when generic features that are part of husky, and this might lead network to predict husky. A neural network in this case would be more trusted if its looks into more deep features that are independednt to environment conditions and does not generalise to both the animals. For the present model, thought the accuracy is 80%, the network cannot be trusted as it is subjected to output wrong result for simple tricked test cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiment was originally conducted as part of the work. [\"Why Should I Trust You?\"](http://www.arxiv.org/abs/1602.04938): Explaining the Predictions of Any Classifier. \n",
    "Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin. In: ACM SIGKDD International Conference on Knowledge Discovery and Data Mining \n",
    "    \n",
    "[Marco](https://homes.cs.washington.edu/~marcotcr/) was kindful enough to share the images used in the experiment"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
